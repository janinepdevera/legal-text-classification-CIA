{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-label Legal Text Classification for CIA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install translators\n",
    "!pip install transformers\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import nltk\n",
    "from functions.source_parsing import *\n",
    "from multiprocessing import Pool\n",
    "from tqdm import *\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/janinedevera/Documents/School/MDS 2021-2023/Thesis/multilabel-legal-text-classification-CIA'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"../..\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_labels_grouped = pd.read_csv(\"data/01 legal_texts_with_labels_grouped.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2256"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_labels_grouped['Length'] = text_labels_grouped['Text'].str.len()\n",
    "text_labels_grouped['Length'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentences(text):\n",
    "    return nltk.sent_tokenize(text)\n",
    "\n",
    "split_text_rows = []\n",
    "for index, row in text_labels_grouped.iterrows():\n",
    "    if row['Length'] > 500:\n",
    "        text = row['Text']\n",
    "        sentences = split_sentences(text)\n",
    "        for sentence in sentences:\n",
    "            if len(sentence) > 500:\n",
    "                chunks = [sentence[i:i+500] for i in range(0, len(sentence), 500)]\n",
    "                for chunk in chunks:\n",
    "                    split_text_rows.append([row['Law'], chunk, row['Category'], row['Category_New']])\n",
    "            else:\n",
    "                split_text_rows.append([row['Law'], sentence, row['Category'], row['Category_New']])\n",
    "    else:\n",
    "        split_text_rows.append([row['Law'], row['Text'], row['Category'], row['Category_New']])\n",
    "\n",
    "text_labels_split = pd.DataFrame(split_text_rows, columns=[\"Law\", \"Text\", \"Category\", \"Category_New\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_labels_split['Length'] = text_labels_split['Text'].str.len()\n",
    "text_labels_split = text_labels_split.loc[text_labels_split['Length'] >= 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A         1251\n",
       "None       750\n",
       "B          227\n",
       "Others      71\n",
       "C           59\n",
       "Name: Category_New, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_labels_split.Category_New.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_labels_split.to_csv(\"data/01 legal_texts_with_labels_grouped_split.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 1650\n",
      "Test set size: 708\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(text_labels_split, test_size=0.3, random_state=999, stratify=text_labels_split['Category_New'])\n",
    "\n",
    "print(f\"Train set size: {len(train_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Back Translation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download language translators\n",
    "def get_translator(lang, from_eng = True):\n",
    "    if from_eng:\n",
    "        translator = ('Helsinki-NLP/opus-mt-en-' + lang)\n",
    "        translator_tokenizer = MarianTokenizer.from_pretrained(translator)\n",
    "        translator_model = MarianMTModel.from_pretrained(translator)\n",
    "        print('en to ' + lang + ' translator downloaded')\n",
    "    else:\n",
    "        translator = ('Helsinki-NLP/opus-mt-' + lang + '-en')\n",
    "        translator_tokenizer = MarianTokenizer.from_pretrained(translator)\n",
    "        translator_model = MarianMTModel.from_pretrained(translator)\n",
    "        print(lang + ' to eng translator downloaded')\n",
    "    return translator_model, translator_tokenizer\n",
    "\n",
    "# translate text\n",
    "def run_translation(batch_text, model, tokenizer, language):\n",
    "    formated_batch_texts = [\">>{}<< {}\".format(language, text) for text in batch_text]\n",
    "    translated = model.generate(**tokenizer(formated_batch_texts, return_tensors=\"pt\", padding=True))\n",
    "    translated_texts = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n",
    "    return translated_texts\n",
    "\n",
    "# back translation\n",
    "def back_translation(model, tokenizer, model_back, tokenizer_back, batch_texts, original_language=\"en\", temporary_language=\"fr\"):\n",
    "  temp_translated_batch = run_translation(batch_texts, model, tokenizer, temporary_language)\n",
    "  back_translated_batch = run_translation(temp_translated_batch, model_back, tokenizer_back, original_language)\n",
    "  return back_translated_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# languages\n",
    "langs = ['ar', 'bg', 'cs', 'da', 'nl', 'fi',  \n",
    "         'fr', 'de', 'hi', 'cy', 'id', \n",
    "         'it', 'ru', 'es', 'sv', 'sk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/janinedevera/opt/miniconda3/envs/mtc-models/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en to ar translator downloaded\n",
      "en to bg translator downloaded\n",
      "en to cs translator downloaded\n",
      "en to da translator downloaded\n",
      "en to nl translator downloaded\n",
      "en to fi translator downloaded\n",
      "en to fr translator downloaded\n",
      "en to de translator downloaded\n",
      "en to hi translator downloaded\n",
      "en to cy translator downloaded\n",
      "en to id translator downloaded\n",
      "en to it translator downloaded\n",
      "en to ru translator downloaded\n",
      "en to es translator downloaded\n",
      "en to sv translator downloaded\n",
      "en to sk translator downloaded\n",
      "ar to eng translator downloaded\n",
      "bg to eng translator downloaded\n",
      "cs to eng translator downloaded\n",
      "da to eng translator downloaded\n",
      "nl to eng translator downloaded\n",
      "fi to eng translator downloaded\n",
      "fr to eng translator downloaded\n",
      "de to eng translator downloaded\n",
      "hi to eng translator downloaded\n",
      "cy to eng translator downloaded\n",
      "id to eng translator downloaded\n",
      "it to eng translator downloaded\n",
      "ru to eng translator downloaded\n",
      "es to eng translator downloaded\n",
      "sv to eng translator downloaded\n",
      "sk to eng translator downloaded\n"
     ]
    }
   ],
   "source": [
    "models = {}\n",
    "tokenizers = {}\n",
    "for lang in langs:\n",
    "    model, tokenizer = get_translator(lang, from_eng=True)\n",
    "    models[lang] = model\n",
    "    tokenizers[lang] = tokenizer\n",
    "\n",
    "models_back = {}\n",
    "tokenizers_back = {}\n",
    "for lang in langs:\n",
    "    model, tokenizer = get_translator(lang, from_eng=False)\n",
    "    models_back[lang] = model\n",
    "    tokenizers_back[lang] = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/janinedevera/opt/miniconda3/envs/mtc-models/lib/python3.8/site-packages/transformers/generation/utils.py:1288: UserWarning: Using `max_length`'s default (512) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 sentences translated. Time elapsed: 2 mins 35 secs.\n",
      "100 sentences translated. Time elapsed: 7 mins 47 secs.\n",
      "150 sentences translated. Time elapsed: 17 mins 44 secs.\n",
      "200 sentences translated. Time elapsed: 25 mins 4 secs.\n",
      "250 sentences translated. Time elapsed: 30 mins 10 secs.\n",
      "300 sentences translated. Time elapsed: 36 mins 52 secs.\n",
      "350 sentences translated. Time elapsed: 42 mins 40 secs.\n",
      "400 sentences translated. Time elapsed: 46 mins 49 secs.\n",
      "450 sentences translated. Time elapsed: 54 mins 25 secs.\n",
      "500 sentences translated. Time elapsed: 58 mins 30 secs.\n",
      "550 sentences translated. Time elapsed: 61 mins 47 secs.\n",
      "600 sentences translated. Time elapsed: 68 mins 2 secs.\n",
      "650 sentences translated. Time elapsed: 68 mins 27 secs.\n",
      "700 sentences translated. Time elapsed: 73 mins 15 secs.\n",
      "750 sentences translated. Time elapsed: 77 mins 0 secs.\n",
      "800 sentences translated. Time elapsed: 83 mins 53 secs.\n",
      "850 sentences translated. Time elapsed: 90 mins 43 secs.\n",
      "900 sentences translated. Time elapsed: 99 mins 17 secs.\n",
      "950 sentences translated. Time elapsed: 108 mins 31 secs.\n",
      "1000 sentences translated. Time elapsed: 110 mins 43 secs.\n",
      "1050 sentences translated. Time elapsed: 117 mins 11 secs.\n",
      "1100 sentences translated. Time elapsed: 120 mins 42 secs.\n",
      "1150 sentences translated. Time elapsed: 130 mins 23 secs.\n",
      "1200 sentences translated. Time elapsed: 139 mins 58 secs.\n",
      "1250 sentences translated. Time elapsed: 148 mins 2 secs.\n",
      "1300 sentences translated. Time elapsed: 154 mins 19 secs.\n",
      "1350 sentences translated. Time elapsed: 158 mins 19 secs.\n",
      "1400 sentences translated. Time elapsed: 159 mins 41 secs.\n",
      "1450 sentences translated. Time elapsed: 167 mins 7 secs.\n",
      "1500 sentences translated. Time elapsed: 174 mins 50 secs.\n",
      "1550 sentences translated. Time elapsed: 180 mins 1 secs.\n",
      "1600 sentences translated. Time elapsed: 182 mins 46 secs.\n",
      "1650 sentences translated. Time elapsed: 189 mins 23 secs.\n"
     ]
    }
   ],
   "source": [
    "back_translated_rows = []\n",
    "num_sentences_translated = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for index, row in train_df.iterrows():\n",
    "    law = row[\"Law\"]\n",
    "    category = row[\"Category\"]\n",
    "    category_new = row[\"Category_New\"]\n",
    "    text = row[\"Text\"]\n",
    "    \n",
    "    if category_new == \"A\":\n",
    "        back_translated_text_set = [text]\n",
    "\n",
    "    elif category_new == \"B\":\n",
    "        lang_choices = random.sample(langs, 5)\n",
    "        back_translated_text_set = [text]\n",
    "\n",
    "        for lang in lang_choices:\n",
    "            model = models[lang]\n",
    "            tokenizer = tokenizers[lang]\n",
    "            model_back = models_back[lang]\n",
    "            tokenizer_back = tokenizers_back[lang]\n",
    "            temp_translated_batch = run_translation([text], model, tokenizer, lang)\n",
    "            back_translated_text_set += run_translation(temp_translated_batch, model_back, tokenizer_back, \"en\")\n",
    "            \n",
    "    elif category_new in [\"C\", \"Others\"]:\n",
    "        back_translated_text_set = [text]\n",
    "\n",
    "        for lang in langs:\n",
    "            model = models[lang]\n",
    "            tokenizer = tokenizers[lang]\n",
    "            model_back = models_back[lang]\n",
    "            tokenizer_back = tokenizers_back[lang]\n",
    "            temp_translated_batch = run_translation([text], model, tokenizer, lang)\n",
    "            back_translated_text_set += run_translation(temp_translated_batch, model_back, tokenizer_back, \"en\")\n",
    "    else:\n",
    "        back_translated_text_set = [text]\n",
    "    \n",
    "    for back_translated_text in back_translated_text_set:\n",
    "        back_translated_rows.append([law, back_translated_text, category, category_new])\n",
    "    \n",
    "    # counter\n",
    "    num_sentences_translated += 1\n",
    "    if num_sentences_translated % 50 == 0:\n",
    "        #print(f\"{num_sentences_translated} sentences translated\")\n",
    "        elapsed_time = time.time() - start_time\n",
    "        elapsed_minutes = int(elapsed_time // 60)\n",
    "        elapsed_seconds = int(elapsed_time % 60)\n",
    "        print(f\"{num_sentences_translated} sentences translated. Time elapsed: {elapsed_minutes} mins {elapsed_seconds} secs.\")\n",
    "\n",
    "back_translated_df = pd.DataFrame(back_translated_rows, columns=[\"Law\", \"Text\", \"Category\", \"Category_New\"])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-process train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "B         954\n",
       "A         875\n",
       "Others    850\n",
       "C         697\n",
       "None      525\n",
       "Name: Category_New, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "back_translated_df.Category_New.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_translated_df['text_clean'] = preprocess_corpus_keep_stop_words(back_translated_df['Text'])\n",
    "back_translated_df['text_clean'] = [stem_lemmatize(text) for text in back_translated_df['text_clean']]\n",
    "\n",
    "back_translated_df = back_translated_df.drop_duplicates(subset=['Text', 'Category_New'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['text_clean'] = preprocess_corpus_keep_stop_words(test_df['Text'])\n",
    "test_df['text_clean'] = [stem_lemmatize(text) for text in test_df['text_clean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_translated_df.to_csv(\"data/01 train_data_augmented.csv\")\n",
    "test_df.to_csv(\"data/01 test_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mtc-models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
