{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-label Legal Text Classification for CIA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models and Experiments: Adaptive Pre-Training with Sentence BERT Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import csv\n",
    "import gzip\n",
    "import random\n",
    "import time\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/janinedevera/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/janinedevera/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "/Users/janinedevera/opt/miniconda3/envs/mtc-models/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from functions.source_parsing import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, precision_score, recall_score\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel, BertTokenizerFast, BertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW\n",
    "from sentence_transformers import models, losses, datasets\n",
    "from sentence_transformers import LoggingHandler, SentenceTransformer, util, InputExample\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from sentence_transformers.cross_encoder.evaluation import CESoftmaxAccuracyEvaluator\n",
    "from sentence_transformers.datasets import DenoisingAutoEncoderDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset, RandomSampler, SequentialSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "import math\n",
    "import logging\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../..\")\n",
    "os.getcwd()\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data (by sentence)\n",
    "text = pd.read_csv(\"data/01 legal_texts_pipeline_sentence.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sentences\n",
    "text['sentence_clean'] = text['sentence_clean'].astype(str)\n",
    "text_list = text['sentence_clean'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "oecd_defs = pd.read_csv(\"data/02 oecd_definitions_stopwords_grouped.csv\", index_col=0).rename(columns={'text_clean': 'defs_text'}) # oecd definitions\n",
    "oecd_defs.at[3, 'Main'] = 'Others' # change category D to \"Others\"\n",
    "\n",
    "train_df_augmented = pd.read_csv(\"data/01 train_data_augmented.csv\", index_col=0).rename(columns={'text_clean': 'legal_text'}) # labeled training data augmented\n",
    "test_df = pd.read_csv(\"data/01 test_data.csv\", index_col=0).rename(columns={'text_clean': 'legal_text'}) # labeled test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_augmented['legal_text'] = preprocess_corpus_keep_stop_words(train_df_augmented['Text'])\n",
    "train_df_augmented['legal_text'] = [lemmatize(text) for text in train_df_augmented['legal_text']]\n",
    "\n",
    "test_df['legal_text'] = preprocess_corpus_keep_stop_words(test_df['Text'])\n",
    "test_df['legal_text'] = [lemmatize(text) for text in test_df['legal_text']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Base BERT Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code from this section is based on https://github.com/janinepdevera/document-classification-BERT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. BERT for binary classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create binary_label column\n",
    "train_df_augmented['binary_label'] = train_df_augmented['Category_New'].apply(lambda x: 0 if x == \"None\" else 1)\n",
    "train_df_augmented.drop_duplicates(subset=('legal_text', 'binary_label'), keep='first')\n",
    "\n",
    "test_df['binary_label'] = test_df['Category_New'].apply(lambda x: 0 if x == \"None\" else 1)\n",
    "test_df_binary = test_df.drop_duplicates(subset=('legal_text', 'binary_label'), keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at nlpaueb/legal-bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# bert-base-uncased\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased', return_dict=False)\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased', return_dict=False)\n",
    "\n",
    "# legal-bert-base-uncased\n",
    "legal_bert = AutoModel.from_pretrained(\"nlpaueb/legal-bert-base-uncased\", return_dict=False)\n",
    "legal_tokenizer = BertTokenizerFast.from_pretrained(\"nlpaueb/legal-bert-base-uncased\", return_dict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_sample = train_df_augmented.sample(500)\n",
    "test_df_sample = test_df_binary.sample(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    190\n",
       "0    110\n",
       "Name: binary_label, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_sample['binary_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = train_df_augmented['legal_text'].to_list()\n",
    "test_text = test_df['legal_text'].to_list()\n",
    "\n",
    "train_labels = train_df_augmented['binary_label'].to_list()\n",
    "test_labels = test_df['binary_label'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/janinedevera/opt/miniconda3/envs/mtc-models/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# tokenize and encode sequences in the training set\n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    train_text,\n",
    "    max_length = 512,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the test set\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    test_text,\n",
    "    max_length = 512,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert lists to tensors\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(train_labels)\n",
    "\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "test_y = torch.tensor(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "num_workers = 2\n",
    "\n",
    "# dataLoader for train set\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "train_dataloader = DataLoader(train_data, num_workers=num_workers, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "# dataLoader for test set\n",
    "test_data = TensorDataset(test_seq, test_mask, test_y)\n",
    "test_dataloader = DataLoader(test_data, num_workers=num_workers, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_Arch(nn.Module):\n",
    "\n",
    "    def __init__(self, bert):\n",
    "      \n",
    "      super(BERT_Arch, self).__init__()\n",
    "\n",
    "      self.bert = bert \n",
    "      self.dropout = nn.Dropout(0.3)\n",
    "      self.relu =  nn.ReLU()\n",
    "      self.fc1 = nn.Linear(768,512)\n",
    "      self.fc2 = nn.Linear(512,1)\n",
    "      #self.softmax = nn.LogSoftmax(dim=1)\n",
    "      self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, sent_id, mask):\n",
    "\n",
    "      #pass the inputs to the model  \n",
    "      _, cls_hs = self.bert(sent_id, attention_mask=mask)\n",
    "      x = self.fc1(cls_hs)\n",
    "      x = self.relu(x)\n",
    "      x = self.dropout(x)\n",
    "      x = self.fc2(x)\n",
    "      #x = self.softmax(x)\n",
    "      x = self.sigmoid(x)\n",
    "\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to freeze all the parameters if freeze = T\n",
    "def set_parameter_requires_grad(model, freeze):\n",
    "    if freeze:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_parameter_requires_grad(model=bert, freeze=True)\n",
    "bert_classifier = BERT_Arch(bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer):\n",
    "  model.train()\n",
    "  total_loss = 0\n",
    "  total_preds = []\n",
    "  total_labels = []\n",
    "  \n",
    "  for inputs in tqdm(dataloader):\n",
    "    \n",
    "    # push to gpu\n",
    "    inputs = [r.to(device) for r in inputs]\n",
    "    sent_id, mask, labels = inputs\n",
    "\n",
    "    # zero the parameter gradients\n",
    "    model.zero_grad()        \n",
    "\n",
    "    # forward + backward + optimize \n",
    "    preds = model(sent_id, mask)\n",
    "    #loss = criterion(preds, labels)\n",
    "    loss = criterion(preds.view(-1), labels.float())\n",
    "    total_loss += loss.item()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) #prevent exploding gradient problem\n",
    "    optimizer.step()\n",
    "\n",
    "    total_labels.append(labels)\n",
    "    #total_preds.append(preds.argmax(dim=-1))\n",
    "    total_preds.append(preds)\n",
    "\n",
    "  total_labels = torch.cat(total_labels)\n",
    "  total_preds = torch.cat(total_preds)\n",
    "  total_preds = (total_preds > 0.5).float()\n",
    "  \n",
    "  # epoch loss and accuracy\n",
    "  epoch_loss = total_loss / len(dataloader)\n",
    "  epoch_acc = accuracy_score(total_labels.detach().numpy(), total_preds.detach().numpy().argmax(axis=1))\n",
    "  #epoch_f1 = f1_score(total_labels, total_preds, average='weighted')\n",
    "\n",
    "  return epoch_loss, epoch_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion):\n",
    "  model.eval()\n",
    "  total_loss = 0\n",
    "  total_preds = []\n",
    "  total_labels = []\n",
    "\n",
    "  for inputs in tqdm(dataloader):\n",
    "    \n",
    "    # push to gpu\n",
    "    inputs = [t.to(device) for t in inputs]\n",
    "    sent_id, mask, labels = inputs\n",
    "\n",
    "    with torch.no_grad():\n",
    "      preds = model(sent_id, mask)\n",
    "      #loss = criterion(preds,labels)\n",
    "      loss = criterion(preds.view(-1), labels.float())\n",
    "      total_loss += loss.item()\n",
    "\n",
    "      total_labels.append(labels)\n",
    "      #total_preds.append(preds.argmax(dim=-1))\n",
    "      total_preds.append(preds)\n",
    "   \n",
    "  total_labels = torch.cat(total_labels)\n",
    "  total_preds = torch.cat(total_preds)\n",
    "  total_preds = (total_preds > 0.5).float()\n",
    "\n",
    "  # epoch loss and model predictions\n",
    "  epoch_loss = total_loss / len(dataloader)\n",
    "  epoch_acc = accuracy_score(total_labels.detach().numpy(), total_preds.detach().numpy().argmax(axis=1))\n",
    "  #epoch_f1 = f1_score(total_labels, total_preds, average='weighted')\n",
    "\n",
    "\n",
    "  return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, criterion, train_loader, val_loader, epochs, path):\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    train_losses=[]\n",
    "    valid_losses=[]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "        train_loss, train_acc = train(model, train_loader, criterion, optimizer)\n",
    "        valid_loss, valid_acc = evaluate(model, val_loader, criterion)\n",
    "        \n",
    "        # save best model\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), path)\n",
    "        \n",
    "        # append training and validation loss\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.2f}\")\n",
    "        print(f\"Validation Loss: {valid_loss:.2f}\")\n",
    "        print(f\"Train Accuracy: {train_acc:.3f}\")\n",
    "        print(f\"Validation Accuracy: {valid_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "learning_rate = 1e-5\n",
    "\n",
    "optimizer = AdamW(bert_classifier.parameters(), lr = learning_rate)\n",
    "criterion  = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 431/431 [40:08<00:00,  5.59s/it]\n",
      "100%|██████████| 89/89 [05:16<00:00,  3.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.49\n",
      "Validation Loss: 0.63\n",
      "Train Accuracy: 0.152\n",
      "Validation Accuracy: 0.318\n"
     ]
    }
   ],
   "source": [
    "fit(bert_classifier, criterion, train_dataloader, test_dataloader, epochs, 'models/bert-'+datetime.now().strftime(\"%Y-%m-%d_%H-%M\")+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'models/bert-2023-05-07_21-24.pt'\n",
    "bert_classifier.load_state_dict(torch.load(path))\n",
    "\n",
    "with torch.no_grad():\n",
    "  preds = bert_classifier(test_seq.to(device), test_mask.to(device))\n",
    "  preds = preds.detach().cpu().numpy()\n",
    "\n",
    "preds = np.argmax(preds, axis = 1)\n",
    "print(classification_report(test_y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate classification report\n",
    "report = classification_report(test_labels, preds, zero_division=0, output_dict=True)\n",
    "\n",
    "# extract precision, recall, and f1-score for each category\n",
    "categories = ['0', '1', '2', '3', '4']\n",
    "metrics = ['precision', 'recall', 'f1-score']\n",
    "data = []\n",
    "for category in categories:\n",
    "    row = []\n",
    "    for metric in metrics:\n",
    "        row.append(round(report[category][metric], 2))\n",
    "    data.append(row)\n",
    "\n",
    "# create dataframe\n",
    "report_df = pd.DataFrame(data, columns=metrics, index=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "custom_cmap_blues = sns.color_palette(['#A9BBD3', '#A0B4CF', '#8FA8C6', '#7E9CBD', '#6D8FB4', '#537CA6', '#416F9D', '#306294', '#0F4A82'])\n",
    "custom_cmap_pinks = sns.color_palette(['#FFE6F2', '#FFCCE4', '#F99FC8', '#F28AB9', '#E177A5', '#D56997', '#C85B89', '#B9537E'])\n",
    "\n",
    "plt.figure(figsize=(6, 8))\n",
    "sns.heatmap(report_df, annot=True, cmap=custom_cmap_blues, cbar=False, vmin=0, vmax=1,  fmt='.2f',\n",
    "            annot_kws={\"fontfamily\": \"Arial\", \"fontsize\": 12})\n",
    "#plt.xlabel('Metrics', labelpad=12, fontname='Arial', fontsize=12)\n",
    "#plt.title('Classification report', fontname='Arial', fontsize=14, y=1.02)\n",
    "plt.xticks(fontname='Arial', fontsize=12)\n",
    "plt.yticks(fontname='Arial', fontsize=12, rotation=0)\n",
    "plt.tick_params(axis='y', length=0)\n",
    "plt.tick_params(axis='x', length=0)\n",
    "\n",
    "# save the figure\n",
    "save_path = os.path.join('draft', 'images', 'plot05-06.png')\n",
    "plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. BERT v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = train_df_augmented['legal_text'].values\n",
    "train_labels = train_df_augmented['binary_label'].values\n",
    "\n",
    "test_text = test_df['legal_text'].values\n",
    "test_labels = test_df['binary_label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/Users/janinedevera/opt/miniconda3/envs/mtc-models/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def preprocessing(input_text, tokenizer):\n",
    "  '''\n",
    "  Returns <class transformers.tokenization_utils_base.BatchEncoding> with the following fields:\n",
    "    - input_ids: list of token ids\n",
    "    - token_type_ids: list of token type ids\n",
    "    - attention_mask: list of indices (0,1) specifying which tokens should considered by the model (return_attention_mask = True).\n",
    "  '''\n",
    "  return tokenizer.encode_plus(\n",
    "                        input_text,\n",
    "                        add_special_tokens = True,\n",
    "                        max_length = 32,\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt'\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_id = []\n",
    "attention_masks = []\n",
    "\n",
    "for sample in train_text:\n",
    "  encoding_dict = preprocessing(sample, tokenizer)\n",
    "  token_id.append(encoding_dict['input_ids']) \n",
    "  attention_masks.append(encoding_dict['attention_mask'])\n",
    "\n",
    "train_token_id = torch.cat(token_id, dim = 0)\n",
    "train_attention_masks = torch.cat(attention_masks, dim = 0)\n",
    "train_labels = torch.tensor(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_id = []\n",
    "attention_masks = []\n",
    "\n",
    "for sample in test_text:\n",
    "  encoding_dict = preprocessing(sample, tokenizer)\n",
    "  token_id.append(encoding_dict['input_ids']) \n",
    "  attention_masks.append(encoding_dict['attention_mask'])\n",
    "\n",
    "test_token_id = torch.cat(token_id, dim = 0)\n",
    "test_attention_masks = torch.cat(attention_masks, dim = 0)\n",
    "test_labels = torch.tensor(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    do_lower_case = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_rand_sentence_encoding():\n",
    "  '''Displays tokens, token IDs and attention mask of a random text sample'''\n",
    "  index = random.randint(0, len(text) - 1)\n",
    "  tokens = tokenizer.tokenize(tokenizer.decode(token_id[index]))\n",
    "  token_ids = [i.numpy() for i in token_id[index]]\n",
    "  attention = [i.numpy() for i in attention_masks[index]]\n",
    "\n",
    "  table = np.array([tokens, token_ids, attention]).T\n",
    "  print(tabulate(table, \n",
    "                 headers = ['Tokens', 'Token IDs', 'Attention Mask'],\n",
    "                 tablefmt = 'fancy_grid'))\n",
    "\n",
    "print_rand_sentence_encoding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels=2, # The number of output labels--2 for binary classification.\n",
    "    output_attentions=False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states=False, # Whether the model returns all hidden-states.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def b_tp(preds, labels):\n",
    "  '''Returns True Positives (TP): count of correct predictions of actual class 1'''\n",
    "  return sum([preds == labels and preds == 1 for preds, labels in zip(preds, labels)])\n",
    "\n",
    "def b_fp(preds, labels):\n",
    "  '''Returns False Positives (FP): count of wrong predictions of actual class 1'''\n",
    "  return sum([preds != labels and preds == 1 for preds, labels in zip(preds, labels)])\n",
    "\n",
    "def b_tn(preds, labels):\n",
    "  '''Returns True Negatives (TN): count of correct predictions of actual class 0'''\n",
    "  return sum([preds == labels and preds == 0 for preds, labels in zip(preds, labels)])\n",
    "\n",
    "def b_fn(preds, labels):\n",
    "  '''Returns False Negatives (FN): count of wrong predictions of actual class 0'''\n",
    "  return sum([preds != labels and preds == 0 for preds, labels in zip(preds, labels)])\n",
    "\n",
    "def b_metrics(preds, labels):\n",
    "  '''\n",
    "  Returns the following metrics:\n",
    "    - accuracy    = (TP + TN) / N\n",
    "    - precision   = TP / (TP + FP)\n",
    "    - recall      = TP / (TP + FN)\n",
    "    - specificity = TN / (TN + FP)\n",
    "  '''\n",
    "  preds = np.argmax(preds, axis = 1).flatten()\n",
    "  labels = labels.flatten()\n",
    "  tp = b_tp(preds, labels)\n",
    "  tn = b_tn(preds, labels)\n",
    "  fp = b_fp(preds, labels)\n",
    "  fn = b_fn(preds, labels)\n",
    "  b_accuracy = (tp + tn) / len(labels)\n",
    "  b_precision = tp / (tp + fp) if (tp + fp) > 0 else 'nan'\n",
    "  b_recall = tp / (tp + fn) if (tp + fn) > 0 else 'nan'\n",
    "  b_specificity = tn / (tn + fp) if (tn + fp) > 0 else 'nan'\n",
    "  return b_accuracy, b_precision, b_recall, b_specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), \n",
    "                              lr = 5e-5,\n",
    "                              eps = 1e-08\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "device = 'cpu'\n",
    "\n",
    "for epoch in trange(epochs, desc = 'Epoch'):\n",
    "    \n",
    "    # ========== Training ==========\n",
    "    \n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Tracking variables\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        train_output = model(b_input_ids, \n",
    "                             token_type_ids = None, \n",
    "                             attention_mask = b_input_mask, \n",
    "                             labels = b_labels)\n",
    "        # Backward pass\n",
    "        train_output.loss.backward()\n",
    "        optimizer.step()\n",
    "        # Update tracking variables\n",
    "        tr_loss += train_output.loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "    # ========== Validation ==========\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    val_accuracy = []\n",
    "    val_precision = []\n",
    "    val_recall = []\n",
    "    val_specificity = []\n",
    "\n",
    "    for batch in test_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "          # Forward pass\n",
    "          eval_output = model(b_input_ids, \n",
    "                              token_type_ids = None, \n",
    "                              attention_mask = b_input_mask)\n",
    "        logits = eval_output.logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        # Calculate validation metrics\n",
    "        b_accuracy, b_precision, b_recall, b_specificity = b_metrics(logits, label_ids)\n",
    "        val_accuracy.append(b_accuracy)\n",
    "        # Update precision only when (tp + fp) !=0; ignore nan\n",
    "        if b_precision != 'nan': val_precision.append(b_precision)\n",
    "        # Update recall only when (tp + fn) !=0; ignore nan\n",
    "        if b_recall != 'nan': val_recall.append(b_recall)\n",
    "        # Update specificity only when (tn + fp) !=0; ignore nan\n",
    "        if b_specificity != 'nan': val_specificity.append(b_specificity)\n",
    "\n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    print('\\n\\t - Train loss: {:.4f}'.format(tr_loss / nb_tr_steps))\n",
    "    print('\\t - Validation Accuracy: {:.4f}'.format(sum(val_accuracy)/len(val_accuracy)))\n",
    "    print('\\t - Validation Precision: {:.4f}'.format(sum(val_precision)/len(val_precision)) if len(val_precision)>0 else '\\t - Validation Precision: NaN')\n",
    "    print('\\t - Validation Recall: {:.4f}'.format(sum(val_recall)/len(val_recall)) if len(val_recall)>0 else '\\t - Validation Recall: NaN')\n",
    "    print('\\t - Validation Specificity: {:.4f}\\n'.format(sum(val_specificity)/len(val_specificity)) if len(val_specificity)>0 else '\\t - Validation Specificity: NaN')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer):\n",
    "  model.train()\n",
    "  total_loss = 0\n",
    "  total_preds = []\n",
    "  total_labels = []\n",
    "  \n",
    "  for inputs in tqdm(dataloader):\n",
    "    \n",
    "    # push to gpu\n",
    "    inputs = [r.to(device) for r in inputs]\n",
    "    sent_id, mask, labels = inputs\n",
    "\n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()        \n",
    "\n",
    "    # forward + backward + optimize \n",
    "    outputs = model(sent_id, attention_mask=mask, labels=labels)\n",
    "    loss = outputs.loss\n",
    "    total_loss += loss.item()\n",
    "    loss.backward()\n",
    "    #torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) #prevent exploding gradient problem\n",
    "    optimizer.step()\n",
    "\n",
    "    total_labels.append(labels)\n",
    "    total_preds.append(outputs.logits)\n",
    "\n",
    "  total_labels = torch.cat(total_labels)\n",
    "  total_preds = torch.cat(total_preds)\n",
    "  total_preds = (total_preds > 0).float()\n",
    "  \n",
    "  # epoch loss and accuracy\n",
    "  epoch_loss = total_loss / len(dataloader)\n",
    "  epoch_acc = accuracy_score(total_labels.detach().numpy(), total_preds.detach().numpy().argmax(axis=1))\n",
    "\n",
    "  return epoch_loss, epoch_acc\n",
    "\n",
    "def evaluate(model, dataloader, criterion):\n",
    "  model.eval()\n",
    "  total_loss = 0\n",
    "  total_preds = []\n",
    "  total_labels = []\n",
    "\n",
    "  for inputs in tqdm(dataloader):\n",
    "    \n",
    "    # push to gpu\n",
    "    inputs = [t.to(device) for t in inputs]\n",
    "    sent_id, mask, labels = inputs\n",
    "\n",
    "    with torch.no_grad():\n",
    "      outputs = model(sent_id, attention_mask=mask, labels=labels)\n",
    "      loss = outputs.loss\n",
    "      total_loss += loss.item()\n",
    "\n",
    "      total_labels.append(labels)\n",
    "      total_preds.append(outputs.logits)\n",
    "   \n",
    "  total_labels = torch.cat(total_labels)\n",
    "  total_preds = torch.cat(total_preds)\n",
    "  total_preds = (total_preds > 0).float()\n",
    "\n",
    "  # epoch loss and model predictions\n",
    "  epoch_loss = total_loss / len(dataloader)\n",
    "  epoch_acc = accuracy_score(total_labels.detach().numpy(), total_preds.detach().numpy().argmax(axis=1))\n",
    "\n",
    "  return epoch_loss, epoch_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, criterion, train_loader, val_loader, epochs, path):\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    train_losses=[]\n",
    "    valid_losses=[]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "        train_loss, train_acc = train(model, train_loader, criterion, optimizer)\n",
    "        valid_loss, valid_acc = evaluate(model, val_loader, criterion)\n",
    "        \n",
    "        # save best model\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), path)\n",
    "        \n",
    "        # append training and validation loss\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.2f}\")\n",
    "        print(f\"Validation Loss: {valid_loss:.2f}\")\n",
    "        print(f\"Train Accuracy: {train_acc:.3f}\")\n",
    "        print(f\"Validation Accuracy: {valid_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/janinedevera/opt/miniconda3/envs/mtc-models/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "learning_rate = 5e-5\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr = learning_rate)\n",
    "criterion  = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [20:05<00:00, 19.14s/it]\n",
      "100%|██████████| 38/38 [02:34<00:00,  4.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.37\n",
      "Validation Loss: 0.53\n",
      "Train Accuracy: 0.818\n",
      "Validation Accuracy: 0.600\n"
     ]
    }
   ],
   "source": [
    "save_path = 'models/bert-'+datetime.now().strftime(\"%Y-%m-%d_%H-%M\")+'.pt'\n",
    "fit(model, criterion, train_dataloader, test_dataloader, epochs, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       120\n",
      "           1       0.60      1.00      0.75       180\n",
      "\n",
      "    accuracy                           0.60       300\n",
      "   macro avg       0.30      0.50      0.37       300\n",
      "weighted avg       0.36      0.60      0.45       300\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/janinedevera/opt/miniconda3/envs/mtc-models/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/janinedevera/opt/miniconda3/envs/mtc-models/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/janinedevera/opt/miniconda3/envs/mtc-models/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "path = save_path\n",
    "model.load_state_dict(torch.load(path))\n",
    "\n",
    "with torch.no_grad():\n",
    "  outputs = model(test_seq.to(device), test_mask.to(device))\n",
    "  logits = outputs.logits.detach().cpu().numpy()\n",
    "  #preds = preds.detach().cpu().numpy()\n",
    "  preds = np.argmax(logits, axis = 1)\n",
    "\n",
    "print(classification_report(test_y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_augmented['Categories'] = train_df_augmented['Category_New'].replace({'None':0, 'A':1, 'B':2, 'C':3, 'Others':4})\n",
    "train_labels = train_df_augmented['Categories'].to_list()\n",
    "test_df['Categories'] = test_df['Category_New'].replace({'A':0, 'B':1, 'C':2, 'Others':3, 'None': 4})\n",
    "test_labels = test_df['Categories'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                      num_labels=5,\n",
    "                                                      output_attentions=False,\n",
    "                                                      output_hidden_states=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "dataloader_train = DataLoader(train_data, \n",
    "                              sampler=RandomSampler(train_data), \n",
    "                              batch_size=batch_size)\n",
    "\n",
    "dataloader_validation = DataLoader(test_data, \n",
    "                                   sampler=SequentialSampler(test_data), \n",
    "                                   batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr=1e-5, \n",
    "                  eps=1e-8)\n",
    "                  \n",
    "epochs = 1\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=0,\n",
    "                                            num_training_steps=len(dataloader_train)*epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def f1_score_func(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average='weighted')\n",
    "\n",
    "def accuracy_per_class(preds, labels):\n",
    "    label_dict_inverse = {v: k for k, v in label_dict.items()}\n",
    "    \n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "\n",
    "    for label in np.unique(labels_flat):\n",
    "        y_preds = preds_flat[labels_flat==label]\n",
    "        y_true = labels_flat[labels_flat==label]\n",
    "        print(f'Class: {label_dict_inverse[label]}')\n",
    "        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "seed_val = 17\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "def evaluate(dataloader_val):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "    \n",
    "    for batch in dataloader_val:\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }\n",
    "\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "    \n",
    "    loss_val_avg = loss_val_total/len(dataloader_val) \n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "            \n",
    "    return loss_val_avg, predictions, true_vals\n",
    "    \n",
    "for epoch in tqdm(range(1, epochs+1)):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    loss_train_total = 0\n",
    "\n",
    "    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
    "    for batch in progress_bar:\n",
    "\n",
    "        model.zero_grad()\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }       \n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        loss_train_total += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "         \n",
    "        \n",
    "    torch.save(model.state_dict(), f'models/finetuned_BERT_epoch_{epoch}.model')\n",
    "        \n",
    "    tqdm.write(f'\\nEpoch {epoch}')\n",
    "    \n",
    "    loss_train_avg = loss_train_total/len(dataloader_train)            \n",
    "    tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "    \n",
    "    val_loss, predictions, true_vals = evaluate(dataloader_validation)\n",
    "    val_f1 = f1_score_func(predictions, true_vals)\n",
    "    tqdm.write(f'Validation loss: {val_loss}')\n",
    "    tqdm.write(f'F1 Score (Weighted): {val_f1}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Sentence BERT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. TSDAE Pre-Training with Unlabeled Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code from this section is based on: https://github.com/UKPLab/sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.backends.mps.is_available())\n",
    "print(torch.backends.mps.is_built())\n",
    "\n",
    "device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model names\n",
    "bert_base = 'bert-base-uncased'\n",
    "distil_roberta = 'distilroberta-base'\n",
    "distil_bert = 'distilbert-base-uncased'\n",
    "legal_bert = 'nlpaueb/legal-bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# initialize model \n",
    "model_name = bert_base\n",
    "word_embedding_model = models.Transformer(model_name) # pre-trained transfomer\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), 'cls') # pooling layer \n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model]) # pre-trained sentence transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': True, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       ")"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to freeze all the parameters if freeze = T\n",
    "def set_parameter_requires_grad(model, freeze):\n",
    "    if freeze:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unfreeze weights to be updated in TSDAE\n",
    "set_parameter_requires_grad(model=model, freeze=False)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader for random sample\n",
    "random.seed(999)\n",
    "text_list_sample = random.sample(text_list, 2000)\n",
    "\n",
    "train_data_sample = DenoisingAutoEncoderDataset(text_list_sample)\n",
    "loader_sample = DataLoader(train_data_sample, batch_size=8, shuffle=True, drop_last=True)\n",
    "\n",
    "# data loader for full data\n",
    "train_data = DenoisingAutoEncoderDataset(text_list)\n",
    "loader = DataLoader(train_data, batch_size=8, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.output.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following encoder weights were not tied to the decoder ['bert/pooler']\n"
     ]
    }
   ],
   "source": [
    "# define denoising autoencoder loss function\n",
    "loss = losses.DenoisingAutoEncoderLoss(model, tie_encoder_decoder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters \n",
    "num_epochs = 3\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pretrain_path = 'models/bert-tsdae-'+datetime.now().strftime(\"%Y-%m-%d_%H-%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training\n",
    "model.fit(\n",
    "    train_objectives=[(loader, loss)],\n",
    "    epochs=num_epochs,\n",
    "    weight_decay=0,\n",
    "    scheduler='constantlr',\n",
    "    optimizer_params={'lr': learning_rate},\n",
    "    show_progress_bar=True,\n",
    "    output_path=model_pretrain_path\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Fine-Tuning: NLI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code from this section is based on: https://github.com/dh1105/Sentence-Entailment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a. Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "oecd_defs = pd.read_csv(\"data/02 oecd_definitions_stopwords_grouped.csv\", index_col=0).rename(columns={'text_clean': 'defs_text'}) # oecd definitions\n",
    "oecd_defs.at[3, 'Main'] = 'Others' # change category D to \"Others\"\n",
    "\n",
    "train_df_augmented = pd.read_csv(\"data/01 train_data_augmented.csv\", index_col=0).rename(columns={'text_clean': 'legal_text'}) # labeled training data augmented\n",
    "test_df = pd.read_csv(\"data/01 test_data.csv\", index_col=0).rename(columns={'text_clean': 'legal_text'}) # labeled test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rd/n9w0gpv53y72x5k9wk3hp63w0000gn/T/ipykernel_7630/762386731.py:5: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  oecd_defs = oecd_defs.append(none, ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Main</th>\n",
       "      <th>defs_text</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>limit the number of supplier lead to the risk ...</td>\n",
       "      <td>833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>regul can affect a supplier 's abil to compet ...</td>\n",
       "      <td>781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C</td>\n",
       "      <td>regul can affect supplier behaviour by not onl...</td>\n",
       "      <td>459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Others</td>\n",
       "      <td>regul sometim limit choic avail to consum for ...</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>This paragraph does not talk about limiting th...</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Main                                          defs_text  length\n",
       "0       A  limit the number of supplier lead to the risk ...     833\n",
       "1       B  regul can affect a supplier 's abil to compet ...     781\n",
       "2       C  regul can affect supplier behaviour by not onl...     459\n",
       "3  Others  regul sometim limit choic avail to consum for ...     257\n",
       "4    None  This paragraph does not talk about limiting th...     217"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add none category to oecd_defs\n",
    "none_description = 'This paragraph does not talk about limiting the number or range of suppliers, limiting the ability of competitors to compete, reducing incentive of suppliers to compete, or limiting the choices available to consumers.'\n",
    "\n",
    "none = {'Main': 'None', 'defs_text': none_description, 'length': len(none_description)}\n",
    "oecd_defs = oecd_defs.append(none, ignore_index=True)\n",
    "oecd_defs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-label dataset in NLI format\n",
    "train_nli_multi = pd.merge(train_df_augmented, oecd_defs, left_on='Category_New', right_on='Main')\n",
    "test_nli_multi = pd.merge(test_df, oecd_defs, left_on='Category_New', right_on='Main')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "B         894\n",
       "Others    743\n",
       "C         662\n",
       "A         624\n",
       "None      522\n",
       "Name: Category_New, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_nli_multi['Category_New'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral       13780\n",
       "entailment     3445\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# binary dataset in NLI format\n",
    "train_nli = train_df_augmented.loc[train_df_augmented.index.repeat(5)].reset_index(drop=True)\n",
    "oecd_defs_nli = pd.concat([oecd_defs] * len(train_df_augmented), ignore_index=True)\n",
    "train_nli = pd.concat([train_nli, oecd_defs_nli], axis=1)\n",
    "train_nli['label'] = np.where(train_nli['Category_New'] == train_nli['Main'], 'entailment', 'neutral') #create labels = 1 for entailment, 0 for neutral\n",
    "train_nli['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral       2832\n",
       "entailment     708\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_nli = test_df.loc[test_df.index.repeat(5)].reset_index(drop=True)\n",
    "oecd_defs_nli = pd.concat([oecd_defs] * len(test_df), ignore_index=True)\n",
    "test_nli = pd.concat([test_nli, oecd_defs_nli], axis=1)\n",
    "test_nli['label'] = np.where(test_nli['Category_New'] == test_nli['Main'], 'entailment', 'neutral') #create labels = 1 for entailment, 0 for neutral\n",
    "test_nli['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "entailment    3445\n",
       "neutral       3000\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# binary dataset in NLI format (balanced)\n",
    "neutral_subset = train_nli.loc[train_nli['label'] == 'neutral'].sample(3000, random_state=999)\n",
    "entailment_subset = train_nli.loc[train_nli['label'] == 'entailment']\n",
    "train_data_balanced = pd.concat([neutral_subset, entailment_subset], ignore_index=True)\n",
    "train_data_balanced['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample data for testing models\n",
    "train_sample = train_nli.sample(200)\n",
    "test_sample = test_nli.sample(100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b. NLI encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset to be used\n",
    "train_df, val_df = train_sample, test_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert df to NLI dclass\n",
    "class NLIData():\n",
    "\n",
    "  def __init__(self, train_df, val_df):\n",
    "    self.label_dict = {'entailment': 0 ,'neutral': 1}\n",
    "\n",
    "    self.train_df = train_df\n",
    "    self.val_df = val_df\n",
    "\n",
    "    self.base_path = '/content/'\n",
    "    self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True) # Using a pre-trained BERT tokenizer to encode sentences\n",
    "    self.train_data = None\n",
    "    self.val_data = None\n",
    "    self.init_data()\n",
    "\n",
    "  def init_data(self):\n",
    "    self.train_data = self.load_data(self.train_df)\n",
    "    self.val_data = self.load_data(self.val_df)\n",
    "\n",
    "  def load_data(self, df):\n",
    "    MAX_LEN = 512\n",
    "    token_ids = []\n",
    "    mask_ids = []\n",
    "    seg_ids = []\n",
    "    y = []\n",
    "\n",
    "    premise_list = df['legal_text'].to_list()\n",
    "    hypothesis_list = df['defs_text'].to_list()\n",
    "    label_list = df['label'].to_list()\n",
    "\n",
    "    # combine premise and hypothesis sequences\n",
    "    for (premise, hypothesis, label) in zip(premise_list, hypothesis_list, label_list):\n",
    "\n",
    "      premise_id = self.tokenizer.encode(premise, add_special_tokens = False, truncation='longest_first', max_length = 254)\n",
    "      hypothesis_id = self.tokenizer.encode(hypothesis, add_special_tokens = False, truncation='longest_first', max_length = 255)\n",
    "\n",
    "      pair_token_ids = [self.tokenizer.cls_token_id] + premise_id + [self.tokenizer.sep_token_id] + hypothesis_id + [self.tokenizer.sep_token_id]\n",
    "      premise_len = len(premise_id)\n",
    "      hypothesis_len = len(hypothesis_id)\n",
    "\n",
    "      segment_ids = torch.tensor([0] * (premise_len + 2) + [1] * (hypothesis_len + 1))  # sentence 0 and sentence 1\n",
    "      attention_mask_ids = torch.tensor([1] * (premise_len + hypothesis_len + 3))  # mask padded values\n",
    "\n",
    "      token_ids.append(torch.tensor(pair_token_ids))\n",
    "      seg_ids.append(segment_ids)\n",
    "      mask_ids.append(attention_mask_ids)\n",
    "      y.append(self.label_dict[label])\n",
    "    \n",
    "    token_ids = pad_sequence(token_ids, batch_first=True)\n",
    "    mask_ids = pad_sequence(mask_ids, batch_first=True)\n",
    "    seg_ids = pad_sequence(seg_ids, batch_first=True)\n",
    "    y = torch.tensor(y)\n",
    "    \n",
    "    dataset = TensorDataset(token_ids, mask_ids, seg_ids, y)\n",
    "    print(len(dataset))\n",
    "    return dataset\n",
    "\n",
    "  def get_data_loaders(self, batch_size=16, shuffle=True):\n",
    "    train_loader = DataLoader(\n",
    "      self.train_data,\n",
    "      shuffle=shuffle,\n",
    "      batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "      self.val_data,\n",
    "      shuffle=shuffle,\n",
    "      batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "nli_dataset = NLIData(train_df, val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data loaders\n",
    "train_loader, val_loader = nli_dataset.get_data_loaders(batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([1, 395]), torch.Size([1, 395]), torch.Size([1, 395]), torch.Size([1])]\n"
     ]
    }
   ],
   "source": [
    "# check dimensions\n",
    "train_loader, _ = nli_dataset.get_data_loaders(batch_size=1, shuffle=False)\n",
    "for batch in train_loader:\n",
    "    print([b.shape for b in batch])\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b. NLI fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('models/tsdae-2023-03-22_12-39/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       ")"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {'entailment': 1 ,'neutral': 0}\n",
    "train_samples = []\n",
    "\n",
    "for row in train_sample.iterrows():\n",
    "    label_id = label_dict[row[1]['label']]\n",
    "    train_samples.append((row[1]['legal_text'], row[1]['defs_text'], label_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_dataloader = DataLoader(train_loader, shuffle=True, batch_size=batch_size)\n",
    "train_loss = losses.SoftmaxLoss(model=model, sentence_embedding_dimension=model.get_sentence_embedding_dimension(), num_labels=len(label_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "\n",
    "model.fit(train_objectives=[(train_loader, train_loss)],\n",
    "          epochs=num_epochs,\n",
    "          show_progress_bar=True,\n",
    "          output_path='models'\n",
    "          )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c. Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Define base and optimizer\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2, max_length=512)\n",
    "#model = SentenceTransformer('models/tsdae-2023-03-22_12-39/')\n",
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for accuracy\n",
    "def multi_acc(y_pred, y_test):\n",
    "  acc = (torch.log_softmax(y_pred, dim=1).argmax(dim=1) == y_test).sum().float() / float(y_test.size(0))\n",
    "  return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training function\n",
    "def train(model, train_loader, optimizer):    \n",
    "  model.train()\n",
    "  total_train_loss = 0\n",
    "  total_train_acc  = 0\n",
    "  \n",
    "  for batch_idx, (pair_token_ids, mask_ids, seg_ids, y) in enumerate(train_loader):\n",
    "    optimizer.zero_grad()\n",
    "    pair_token_ids = pair_token_ids.to(device)\n",
    "    mask_ids = mask_ids.to(device)\n",
    "    seg_ids = seg_ids.to(device)\n",
    "    labels = y.to(device)\n",
    "    \n",
    "    loss, prediction = model(pair_token_ids, \n",
    "                             token_type_ids=seg_ids, \n",
    "                             attention_mask=mask_ids, \n",
    "                             labels=labels).values()\n",
    "\n",
    "    acc = multi_acc(prediction, labels)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "      \n",
    "    total_train_loss += loss.item()\n",
    "    total_train_acc  += acc.item()\n",
    "      \n",
    "  train_acc  = total_train_acc/len(train_loader)\n",
    "  train_loss = total_train_loss/len(train_loader)\n",
    "  \n",
    "  return train_acc, train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation function\n",
    "def eval(model, val_loader, optimizer):\n",
    "    model.eval()\n",
    "    total_val_acc  = 0\n",
    "    total_val_loss = 0\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for batch_idx, (pair_token_ids, mask_ids, seg_ids, y) in enumerate(val_loader):\n",
    "        optimizer.zero_grad()\n",
    "        pair_token_ids = pair_token_ids.to(device)\n",
    "        mask_ids = mask_ids.to(device)\n",
    "        seg_ids = seg_ids.to(device)\n",
    "        labels = y.to(device)\n",
    "        \n",
    "        loss, prediction = model(pair_token_ids, \n",
    "                             token_type_ids=seg_ids, \n",
    "                             attention_mask=mask_ids, \n",
    "                             labels=labels).values()\n",
    "        \n",
    "        acc = multi_acc(prediction, labels)\n",
    "\n",
    "        total_val_loss += loss.item()\n",
    "        total_val_acc  += acc.item()\n",
    "\n",
    "        y_true += labels.cpu().numpy().tolist()\n",
    "        y_pred += torch.argmax(prediction, axis=1).cpu().numpy().tolist()\n",
    "\n",
    "    val_acc  = total_val_acc/len(val_loader)\n",
    "    val_loss = total_val_loss/len(val_loader)\n",
    "\n",
    "    return val_acc, val_loss, y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model by epoch\n",
    "def fit(model, train_loader, val_loader, optimizer, epochs):\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    train_losses=[]\n",
    "    valid_losses=[]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "        train_loss, train_acc = train(model, train_loader, optimizer)\n",
    "        valid_loss, valid_acc, y_true, y_pred = eval(model, val_loader, optimizer)\n",
    "        \n",
    "        # save best model\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), 'models/bert-nli-'+datetime.now().strftime(\"%Y-%m-%d_%H-%M\")+'.pt')\n",
    "        \n",
    "        # append training and validation loss\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.2f}\")\n",
    "        print(f\"Validation Loss: {valid_loss:.2f}\")\n",
    "        print(f\"Train Accuracy: {train_acc:.3f}\")\n",
    "        print(f\"Validation Accuracy: {valid_acc:.3f}\")\n",
    "\n",
    "        print(classification_report(y_true, y_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/janinedevera/opt/miniconda3/envs/mtc-models/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "learning_rate = 1e-3\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr = learning_rate)\n",
    "#criterion = losses.SoftmaxLoss(model=model, sentence_embedding_dimension=model.get_sentence_embedding_dimension(), num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model, train_loader, val_loader, optimizer, epochs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### d. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "\n",
    "def train(model, train_loader, val_loader, optimizer, save_dir):  \n",
    "  total_step = len(train_loader)\n",
    "  best_val_acc = 0.0\n",
    "\n",
    "  for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    total_train_acc  = 0\n",
    "    for batch_idx, (pair_token_ids, mask_ids, seg_ids, y) in enumerate(train_loader):\n",
    "      optimizer.zero_grad()\n",
    "      pair_token_ids = pair_token_ids.to(device)\n",
    "      mask_ids = mask_ids.to(device)\n",
    "      seg_ids = seg_ids.to(device)\n",
    "      labels = y.to(device)\n",
    "\n",
    "      loss, prediction = model(pair_token_ids, \n",
    "                             token_type_ids=seg_ids, \n",
    "                             attention_mask=mask_ids, \n",
    "                             labels=labels).values()\n",
    "\n",
    "      acc = multi_acc(prediction, labels)\n",
    "\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      \n",
    "      total_train_loss += loss.item()\n",
    "      total_train_acc  += acc.item()\n",
    "\n",
    "    train_acc  = total_train_acc/len(train_loader)\n",
    "    train_loss = total_train_loss/len(train_loader)\n",
    "    \n",
    "    model.eval()\n",
    "    total_val_acc  = 0\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "      for batch_idx, (pair_token_ids, mask_ids, seg_ids, y) in enumerate(val_loader):\n",
    "        optimizer.zero_grad()\n",
    "        pair_token_ids = pair_token_ids.to(device)\n",
    "        mask_ids = mask_ids.to(device)\n",
    "        seg_ids = seg_ids.to(device)\n",
    "        labels = y.to(device)\n",
    "        \n",
    "        loss, prediction = model(pair_token_ids, \n",
    "                             token_type_ids=seg_ids, \n",
    "                             attention_mask=mask_ids, \n",
    "                             labels=labels).values()\n",
    "        \n",
    "        acc = multi_acc(prediction, labels)\n",
    "\n",
    "        total_val_loss += loss.item()\n",
    "        total_val_acc  += acc.item()\n",
    "\n",
    "    val_acc  = total_val_acc/len(val_loader)\n",
    "    val_loss = total_val_loss/len(val_loader)\n",
    "    end = time.time()\n",
    "    hours, rem = divmod(end-start, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "\n",
    "    print(f'Epoch {epoch+1}: train_loss: {train_loss:.4f} train_acc: {train_acc:.4f} | val_loss: {val_loss:.4f} val_acc: {val_acc:.4f}')\n",
    "    print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "      best_val_acc = val_acc\n",
    "      save_path = os.path.join('models/', f\"model_epoch_{epoch+1}_val_acc_{val_acc:.4f}.pt\")\n",
    "      #save_path = os.path.join(save_dir, f\"model_epoch_{epoch+1}_val_acc_{val_acc:.4f}.pt\")\n",
    "      torch.save(model.state_dict(), save_path)\n",
    "      print(f\"Saved model checkpoint to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss: 0.5137 train_acc: 0.7914 | val_loss: 0.4895 val_acc: 0.8018\n",
      "03:24:31.20\n",
      "Saved model checkpoint to models/model_epoch_1_val_acc_0.8018.pt\n"
     ]
    }
   ],
   "source": [
    "# Freeze all layers except the last one\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Define the optimizer for the last layer\n",
    "optimizer = torch.optim.Adam(model.classifier.parameters(), lr=1e-5)\n",
    "\n",
    "# Train the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train(model, train_loader, val_loader, optimizer, 'models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import torch\n",
    "\n",
    "# load the saved model\n",
    "#model = BertForSequenceClassification.from_pretrained(\"models/tsdae-2023-03-22_12-39/\", num_labels=2)\n",
    "model.load_state_dict(torch.load('models/model_epoch_1_val_acc_0.8280.pt'))\n",
    "model.eval() # set model to evaluation mode\n",
    "\n",
    "# define a function to get predictions from the model\n",
    "def get_predictions(model, data_loader):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for pair_token_ids, mask_ids, seg_ids, labels in data_loader:\n",
    "        pair_token_ids = pair_token_ids.to(device)\n",
    "        mask_ids = mask_ids.to(device)\n",
    "        seg_ids = seg_ids.to(device)\n",
    "        labels = labels.to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(pair_token_ids, \n",
    "                             token_type_ids=seg_ids, \n",
    "                             attention_mask=mask_ids)\n",
    "            _, predicted = torch.max(outputs.logits.data, 1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "    return y_true, y_pred\n",
    "\n",
    "# get predictions on test data\n",
    "y_true, y_pred = get_predictions(model, val_loader)\n",
    "\n",
    "# compute and print confusion matrix\n",
    "report = classification_report(y_true, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Fine-Tuning: NLI v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Main</th>\n",
       "      <th>defs_text</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>limit the number of supplier lead to the risk ...</td>\n",
       "      <td>833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>regul can affect a supplier 's abil to compet ...</td>\n",
       "      <td>781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C</td>\n",
       "      <td>regul can affect supplier behaviour by not onl...</td>\n",
       "      <td>459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Others</td>\n",
       "      <td>regul sometim limit choic avail to consum for ...</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>This paragraph does not talk about limiting th...</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Main                                          defs_text  length\n",
       "0       A  limit the number of supplier lead to the risk ...     833\n",
       "1       B  regul can affect a supplier 's abil to compet ...     781\n",
       "2       C  regul can affect supplier behaviour by not onl...     459\n",
       "3  Others  regul sometim limit choic avail to consum for ...     257\n",
       "4    None  This paragraph does not talk about limiting th...     217"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oecd_defs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Law</th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "      <th>Category_New</th>\n",
       "      <th>legal_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ordinance NORMAM 12 by DPC-Pilotage</td>\n",
       "      <td>The minimum number of pilotage runs per semest...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>the minimum number of pilotag run per semest c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NOM-026-Z00- 1994, Características y especific...</td>\n",
       "      <td>Establishments subjected to this norm (I.e. th...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>establish subject to thi norm i.e those that m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NOM-059-Z00- 1997, Salud animal. Especificaciones</td>\n",
       "      <td>The norm specifically states that it is not in...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>the norm specif state that it is not in line w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Law 13475/2017 - Aeronaut profession</td>\n",
       "      <td>Duty period limitations for flight and cabin c...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>duti period limit for flight and cabin crew of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Law 7565/1986 - Brazilian Aeronautical Code</td>\n",
       "      <td>The responsible party that pays the indemnity ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>the respons parti that pay the indemn is exone...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Law  \\\n",
       "0                Ordinance NORMAM 12 by DPC-Pilotage   \n",
       "1  NOM-026-Z00- 1994, Características y especific...   \n",
       "2  NOM-059-Z00- 1997, Salud animal. Especificaciones   \n",
       "3               Law 13475/2017 - Aeronaut profession   \n",
       "4        Law 7565/1986 - Brazilian Aeronautical Code   \n",
       "\n",
       "                                                Text Category Category_New  \\\n",
       "0  The minimum number of pilotage runs per semest...        A            A   \n",
       "1  Establishments subjected to this norm (I.e. th...        A            A   \n",
       "2  The norm specifically states that it is not in...        A            A   \n",
       "3  Duty period limitations for flight and cabin c...        A            A   \n",
       "4  The responsible party that pays the indemnity ...     None         None   \n",
       "\n",
       "                                          legal_text  \n",
       "0  the minimum number of pilotag run per semest c...  \n",
       "1  establish subject to thi norm i.e those that m...  \n",
       "2  the norm specif state that it is not in line w...  \n",
       "3  duti period limit for flight and cabin crew of...  \n",
       "4  the respons parti that pay the indemn is exone...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_augmented.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Law</th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "      <th>Category_New</th>\n",
       "      <th>Length</th>\n",
       "      <th>legal_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>Reglamento de la Ley General de Salud en Mater...</td>\n",
       "      <td>The export of narcotics and of medicines / psy...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>100</td>\n",
       "      <td>the export of narcot and of medicin psychotrop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2207</th>\n",
       "      <td>Resolution 192/2011 by National Civil Aviation...</td>\n",
       "      <td>The person, natural or legal, who has contribu...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>256</td>\n",
       "      <td>the person natur or legal who ha contribut fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1344</th>\n",
       "      <td>Regulation (RBHA) 63/2006 by Department of Civ...</td>\n",
       "      <td>rised school).</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>14</td>\n",
       "      <td>rise school</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>NOM-023-Z00- 1995, Identificación de especie a...</td>\n",
       "      <td>The norm specifically states that it is not in...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>144</td>\n",
       "      <td>the norm specif state that it is not in line w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1756</th>\n",
       "      <td>Law 7565/1986 - Brazilian Aeronautical Code</td>\n",
       "      <td>The commander exercises the authority inherent...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>368</td>\n",
       "      <td>the command exercis the author inher to the fu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Law  \\\n",
       "134   Reglamento de la Ley General de Salud en Mater...   \n",
       "2207  Resolution 192/2011 by National Civil Aviation...   \n",
       "1344  Regulation (RBHA) 63/2006 by Department of Civ...   \n",
       "423   NOM-023-Z00- 1995, Identificación de especie a...   \n",
       "1756        Law 7565/1986 - Brazilian Aeronautical Code   \n",
       "\n",
       "                                                   Text Category Category_New  \\\n",
       "134   The export of narcotics and of medicines / psy...        A            A   \n",
       "2207  The person, natural or legal, who has contribu...     None         None   \n",
       "1344                                     rised school).        A            A   \n",
       "423   The norm specifically states that it is not in...        A            A   \n",
       "1756  The commander exercises the authority inherent...     None         None   \n",
       "\n",
       "      Length                                         legal_text  \n",
       "134      100  the export of narcot and of medicin psychotrop...  \n",
       "2207     256  the person natur or legal who ha contribut fin...  \n",
       "1344      14                                        rise school  \n",
       "423      144  the norm specif state that it is not in line w...  \n",
       "1756     368  the command exercis the author inher to the fu...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mtc-models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
